{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# suppress warnings (quite prevalent with pandas and numpy)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# maintain directories well defined\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "ALL_DATA_DIR = \"dat\"\n",
    "DATA_DIR = \"novel-covid-data\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, ALL_DATA_DIR, DATA_DIR)\n",
    "\n",
    "# global variables - desired columns from dataset\n",
    "COLS = [\"SNo\", \"ObservationDate\", \"Province/State\", \"Country/Region\", \"Confirmed\", \"Deaths\"]\n",
    "\n",
    "# function for saving figures\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# function for initialization\n",
    "def initialize_data(dataset, data_path=DATA_PATH, cols=COLS):\n",
    "    csv_path = os.path.join(data_path, dataset)\n",
    "    data = pd.read_csv(csv_path, usecols=cols)\n",
    "    return data\n",
    "\n",
    "# link to data - https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset#covid_19_data.csv\n",
    "# initialize\n",
    "data = initialize_data(\"covid_19_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = data[data[\"Province/State\"]==\"Recovered\"].index\n",
    "data.drop(indexes, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# unstratified split\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified split\n",
    "#split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# for train_index, test_index in split.split(data, data[\"Country/Region\"]):\n",
    "#     strat_train_set = data.loc[train_index]\n",
    "#     strat_test_set = data.loc[test_index]\n",
    "    \n",
    "# strat_train_set\n",
    "\n",
    "country_counts = data[\"Country/Region\"].value_counts()\n",
    "labels = data[\"Country/Region\"].astype('category').cat.categories.tolist()\n",
    "singles = [i for i in labels if country_counts[i] == 1]\n",
    "for i in singles:\n",
    "    indexes = data[data[\"Country/Region\"] == i].index\n",
    "    data.drop(indexes, inplace = True)\n",
    "    \n",
    "country_counts = data[\"Country/Region\"].value_counts()\n",
    "labels = data[\"Country/Region\"].astype('category').cat.categories.tolist()\n",
    "singles = [i for i in labels if country_counts[i] == 1]\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"Country/Region\"]):\n",
    "   strat_train_set = data.loc[train_index]\n",
    "   strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sampling ratios for US as comparison\n",
    "# stratified\n",
    "print(\"Stratified Test: \", strat_test_set[\"Country/Region\"].value_counts()[\"US\"]/len(strat_test_set))\n",
    "print(\"Stratified Train: \", strat_train_set[\"Country/Region\"].value_counts()[\"US\"]/len(strat_train_set))\n",
    "\n",
    "# unstratified\n",
    "print(\"UnStratified Test: \", test_set[\"Country/Region\"].value_counts()[\"US\"]/len(test_set))\n",
    "print(\"UnStratified Train: \", train_set[\"Country/Region\"].value_counts()[\"US\"]/len(train_set))\n",
    "\n",
    "# original data\n",
    "print(\"Original Data: \", data[\"Country/Region\"].value_counts()[\"US\"]/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = strat_train_set.copy()\n",
    "# covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groups = covid_data.groupby(\"Country/Region\")\n",
    "missing_all = []\n",
    "missing_none = []\n",
    "missing_some = []\n",
    "for k, df in country_groups: \n",
    "    if df[\"Province/State\"].isnull().all():\n",
    "        missing_all.append(k)\n",
    "    elif df[\"Province/State\"].isnull().any():\n",
    "        if (~df[\"Province/State\"].isnull()).all():\n",
    "            missing_none.append(k)\n",
    "        else: \n",
    "            missing_some.append(k)\n",
    "\n",
    "# print(missing_all, missing_none, missing_some)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_province(df, col_name=\"Region\"):\n",
    "    df.loc[df[\"Province/State\"].isnull(), col_name] = df[\"Country/Region\"] \n",
    "    df.loc[(~df[\"Province/State\"].isnull()), col_name] = df[\"Province/State\"] \n",
    "    df.drop(\"Country/Region\", axis = 1, inplace = True)\n",
    "    df.drop(\"Province/State\", axis = 1, inplace = True)\n",
    "    \n",
    "# country_to_province(covid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data[\"ObsDate\"] = pd.to_datetime(covid_data[\"ObservationDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_since_first_obs(df, col_name=\"Day_Delta\"):\n",
    "    region_groups = df.groupby(\"Region\")\n",
    "    df[col_name] = np.nan\n",
    "    for k, group in region_groups:\n",
    "        group.sort_values(by=\"ObsDate\", inplace = True)\n",
    "        first_day = group.iloc[0][\"ObsDate\"]\n",
    "        for i in range(len(group)):\n",
    "            df.ix[(group.iloc[i][\"SNo\"]-1), col_name] = (group.iloc[i][\"ObsDate\"] - first_day).days\n",
    "\n",
    "days_since_first_obs(covid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# we're going to rewrite our preprocessing functions as custom Sci-Kit classes \n",
    "# this will work better with other off the shelf Sci-Kit processes\n",
    "\n",
    "class CombineLocations(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_original_regions = True): \n",
    "        self.drop_original_regions = drop_original_regions\n",
    "    def fit(self, df, col_name = \"Region\"):\n",
    "        return self\n",
    "    def transform(self, df, col_name = \"Region\"):\n",
    "        df.loc[df[\"Province/State\"].isnull(), col_name] = df[\"Country/Region\"] \n",
    "        df.loc[(~df[\"Province/State\"].isnull()), col_name] = df[\"Province/State\"]\n",
    "        if self.drop_original_regions:\n",
    "            df.drop(\"Country/Region\", axis = 1, inplace = True)\n",
    "            df.drop(\"Province/State\", axis = 1, inplace = True) \n",
    "\n",
    "combine_locs = CombineLocations()\n",
    "combine_locs.fit(covid_data)\n",
    "combine_locs.transform(covid_data)\n",
    "#covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaysObserved(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_original_dates = True): \n",
    "        self.drop_original_dates = drop_original_dates\n",
    "    def fit(self, X, col_name = \"Day_Delta\"):\n",
    "        return self\n",
    "    def transform(self, X, col_name = \"Day_Delta\"):\n",
    "        X[\"ObsDate\"] = pd.to_datetime(X[\"ObservationDate\"])\n",
    "        region_groups = X.groupby(\"Region\")\n",
    "        X[col_name] = np.nan\n",
    "        for k, group in region_groups:\n",
    "            group.sort_values(by = \"ObsDate\", inplace=True)\n",
    "            first_day = group.iloc[0][\"ObsDate\"]\n",
    "            for i in range(len(group)):\n",
    "                X.ix[(group.iloc[i][\"SNo\"]-1), col_name] = (group.iloc[i][\"ObsDate\"] - first_day).days\n",
    "        X.drop(\"ObsDate\", axis = 1, inplace = True)\n",
    "        if self.drop_original_dates:\n",
    "            X.drop(\"ObservationDate\", axis = 1, inplace = True)\n",
    "                \n",
    "days_observed = DaysObserved(drop_original_dates = False)\n",
    "days_observed.fit(covid_data)\n",
    "days_observed.transform(covid_data)\n",
    "#covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# define a list of numerical attributes and categorical attributes\n",
    "num_attributes = [\"Confirmed\", \"Day_Delta\", \"Deaths\", \"SNo\"]\n",
    "cat_attributes = [\"ObservationDate\", \"Region\"]\n",
    "\n",
    "scatter_matrix(covid_data[num_attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer -> fillnas based on a policy and save statistics for later use (numerical features)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# imputer is only for numerical features so lets try it out\n",
    "covid_data_num = covid_data.copy()\n",
    "covid_data_num.drop(cat_attributes, axis = 1, inplace = True)\n",
    "\n",
    "# in this case, the .fit method is simply calculating the medians of each feature\n",
    "imputer.fit(covid_data_num)\n",
    "\n",
    "# imputer also saves statistics (median) as this will be of use if we need to transform new incoming data\n",
    "# we won't have to re-fit the imputer instance\n",
    "# some of our features may not even have had missing values, but the imputer applied the computation anyways\n",
    "# print(imputer.statistics_)\n",
    "\n",
    "# the transform method (which actually fills nas) returns a numpy array\n",
    "covid_data_num_arr = imputer.transform(covid_data_num)\n",
    "covid_data_num = pd.DataFrame(covid_data_num_arr, columns=covid_data_num.columns, index=covid_data_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# the onehotencoder turns categorical variables in to binary values for each possible value of a feature \n",
    "# this avoids \"confusion\" on the model's part, regarding distance related relationships \n",
    "\n",
    "covid_data.dropna(inplace = True)\n",
    "onehot_encoder = OneHotEncoder()\n",
    "covid_data_cat = covid_data.copy()\n",
    "covid_data_cat.drop(num_attributes, axis = 1, inplace = True)\n",
    "\n",
    "covid_data_cat1H = onehot_encoder.fit_transform(covid_data_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# standard scaler -> scale numerical features with scaling/standardization/normalization methods\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "covid_data_num_array = std_scaler.fit_transform(covid_data_num)\n",
    "covid_data_num = pd.DataFrame(covid_data_num_arr, columns=covid_data_num.columns, index=covid_data_num.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
